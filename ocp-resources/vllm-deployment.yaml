apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: vllm
  name: vllm
  namespace: lora
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: vllm
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: vllm
    spec:
      affinity: {}
      containers:
      - command:
          - python
          - -m
          - vllm.entrypoints.openai.api_server
          - --port=8080
          - --model=/models-cache/models/granite-3.0-8b-instruct
          - --served-model-name=granite-3.0-8b-instruct
          - --max-model-len=4096
          - --dtype=bfloat16
          - --max-lora-rank=64
          - --enable-lora
          - --lora-modules
          - english-quotes=/models-cache/models/english-quotes-ibm-granite-3.8b-instruct
          - java-code=/models-cache/models/java-code-ibm-granite-3.8b-instruct
          - emojis=/models-cache/models/emojis-ibm-granite-3.8b-instruct
          - --chat-template=/models-cache/models/chat_template.jinja
          - --uvicorn-log-level=debug
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              key: HUGGING_FACE_HUB_TOKEN
              name: vllm-secrets
        - name: HF_HUB_OFFLINE
          value: "0"
        - name: VLLM_LOGGING_LEVEL
          value: DEBUG
        - name: HF_HOME
          value: /models-cache
        - name: NUMBA_CACHE_DIR
          value: /tmp
        - name: HOME
          value: /models-cache
        image: quay.io/modh/vllm:rhoai-2.16-cuda
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 100
          successThreshold: 1
          timeoutSeconds: 8
        name: server
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: "8"
            memory: 24Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "6"
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          seccompProfile:
            type: RuntimeDefault
        startupProbe:
          failureThreshold: 24
          httpGet:
            path: /health
            port: http
            scheme: HTTP
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 1
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /models-cache
          name: models-cache
        - mountPath: /dev/shm
          name: shm
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 120
      tolerations:
      - effect: NoSchedule
        key: nvidia.com/gpu
        operator: Exists
      volumes:
      - name: models-cache
        persistentVolumeClaim:
          claimName: vllm-models-cache
      - emptyDir:
          medium: Memory
          sizeLimit: 1Gi
        name: shm